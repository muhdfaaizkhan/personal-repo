{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 5px; height: 50px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP\n",
    "\n",
    "### Project Title: Generative AI and Art - understanding and predicting chatter from online communities\n",
    "\n",
    "**DSI-41 Group 2**: Muhammad Faaiz Khan, Lionel Foo, Gabriel Tan\n",
    "\n",
    "## Part 2: Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Imports\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for EDA, modelling and analysis.\n",
    "import pandas as pd\n",
    "import re\n",
    "from emoji import demojize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# this setting widens how many characters pandas will display in a column:\n",
    "pd.options.display.max_colwidth = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimporting the dataframe from scraping in Section 1\n",
    "reddit_df = pd.read_csv('../data/reddit_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Filtering empty/automated posts\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7789 entries, 0 to 7788\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   subr-def_ai   7789 non-null   int64 \n",
      " 1   is_op         7789 non-null   int64 \n",
      " 2   author        7316 non-null   object\n",
      " 3   post_id       7789 non-null   object\n",
      " 4   body          7789 non-null   object\n",
      " 5   upvotes       7789 non-null   int64 \n",
      " 6   num_comments  7789 non-null   int64 \n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 426.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataframe info\n",
    "reddit_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Null and \"[deleted]\" entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the info above, there are 473 empty entries for `author`. This occurs when the author of the post has deleted their account at the time of viewing/scraping. This is acceptable for the prediction model as we will not be using this feature for prediction. For the purpose of analysis, we will relabel empty entries for `author` as \"[deleted]\".\n",
    "\n",
    "Before renaming these `author` entries, we will first check for the number of posts where the entry for `body` is \"[deleted]\". This indicates that the post has been deleted from the subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 111 entries, 61 to 7517\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   subr-def_ai   111 non-null    int64 \n",
      " 1   is_op         111 non-null    int64 \n",
      " 2   author        0 non-null      object\n",
      " 3   post_id       111 non-null    object\n",
      " 4   body          111 non-null    object\n",
      " 5   upvotes       111 non-null    int64 \n",
      " 6   num_comments  111 non-null    int64 \n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 6.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Checking the info for rows where the contents of the post are \"[deleted]\"\n",
    "test = reddit_df.loc[reddit_df['body']=='[deleted]',:]\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 111 rows where the post contents are \"[deleted]\". The `author` feature for these rows are also all null values. These rows are not useful for modelling purposes and will be removed from the dataframe.\n",
    "\n",
    "Posts with the content \"[removed]\" will also be removed from the dataframe. These posts were removed by the moderation team and will not be useful for our modelling/analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out all rows where the the contents of the post are \"[deleted]\"\n",
    "reddit_df = reddit_df.loc[reddit_df['body']!='[deleted]',:].reset_index(drop=True)\n",
    "\n",
    "# Filtering out all rows where the the contents of the post are \"[removed]\"\n",
    "reddit_df = reddit_df.loc[reddit_df['body']!='[removed]',:].reset_index(drop=True)\n",
    "\n",
    "# Relabelling null entries under 'author' as \"[deleted]\"\n",
    "reddit_df['author'].fillna(value='[deleted]', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Filtering Automated Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to filter out the automated posts from Reddit bots/moderators. We will filter these posts by the entries in `author`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of known bots/automated posts between both subreddits\n",
    "bot_list = ['DefendingAIArt-ModTeam',\n",
    "            'AutoModerator',\n",
    "            'WikiSummarizerBot',\n",
    "            'BookFinderBot',\n",
    "            'sneakpeekbot',\n",
    "            'Anti-ThisBot-IB',\n",
    "            'exclaim_bot',\n",
    "            'of_patrol_bot',\n",
    "            'AmputatorBot',\n",
    "            'savevideobot',\n",
    "            'RemindMeBot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7485 entries, 0 to 7484\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   subr-def_ai   7485 non-null   int64 \n",
      " 1   is_op         7485 non-null   int64 \n",
      " 2   author        7485 non-null   object\n",
      " 3   post_id       7485 non-null   object\n",
      " 4   body          7485 non-null   object\n",
      " 5   upvotes       7485 non-null   int64 \n",
      " 6   num_comments  7485 non-null   int64 \n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 409.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Filtering out the automated posts in our dataframe\n",
    "for bot in bot_list:\n",
    "    reddit_df = reddit_df.loc[reddit_df['author']!=bot,:].reset_index(drop=True)\n",
    "\n",
    "# Checking updated dataframe\n",
    "reddit_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Filtering posts affected by redact.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an attempt at running WordVectorizer, the specific sentence below was found to be recurring multiple times in the dataframe:\n",
    "\n",
    "    this message was mass deleted/edited with redact.dev\n",
    "\n",
    "Upon investigation, posts containing this sentence appear to be meaningess strings of text. This is a result of the post being removed from Reddit through [redact.dev](https://redact.dev/), an online service used to mass delete a user's internet posts. We will remove these posts from our dataframe as they do not provide meaningful contribution to the model/analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75                    versed imagine decide bike gaze physical fear shocking fragile capable ` this message was mass deleted/edited with redact.dev `\n",
      "120                          heavy cake touch governor dog coherent smoggy joke hurry quaint ` this message was mass deleted/edited with redact.dev `\n",
      "124                                    wipe pet juggle boast fact like late label aback full ` this message was mass deleted/edited with redact.dev `\n",
      "3518              cooperative compare fuel groovy hard-to-find modern shame pet snails aloof ` this message was mass deleted/edited with redact.dev `\n",
      "3535                    recognise marble dazzling grab public innate crush plate tease point ` this message was mass deleted/edited with redact.dev `\n",
      "3601                     cows disagreeable soup rhythm grey bear plant truck mindless narrow ` this message was mass deleted/edited with redact.dev `\n",
      "3954                         rhythm pathetic rotten full wrench tap tease juggle touch scale ` this message was mass deleted/edited with redact.dev `\n",
      "4016    seemly familiar narrow practice adjoining society bells support materialistic public ` this message was mass deleted/edited with redact.dev `\n",
      "4297                  unwritten encourage tie fact nose subsequent berserk versed deer erect ` this message was mass deleted/edited with redact.dev `\n",
      "4319             humor ghost compare flowery gullible consist aromatic boast shaggy towering ` this message was mass deleted/edited with redact.dev `\n",
      "5019           coordinated advise whistle deer plant impolite cow disagreeable tie combative ` this message was mass deleted/edited with redact.dev `\n",
      "5028                  growth station sheet head puzzled paint intelligent hungry snails cats ` this message was mass deleted/edited with redact.dev `\n",
      "Name: body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define the string to search for\n",
    "string_to_search = 'this message was mass deleted/edited with redact.dev'\n",
    "\n",
    "# Use boolean indexing to filter rows containing the specified string\n",
    "rows_with_string = reddit_df[reddit_df['body'].str.contains(re.escape(string_to_search))]\n",
    "\n",
    "# Display the rows\n",
    "print(rows_with_string['body'])\n",
    "\n",
    "# Refer to the posts printed in the output below for an example of the meaningless strings of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7473 entries, 0 to 7484\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   subr-def_ai   7473 non-null   int64 \n",
      " 1   is_op         7473 non-null   int64 \n",
      " 2   author        7473 non-null   object\n",
      " 3   post_id       7473 non-null   object\n",
      " 4   body          7473 non-null   object\n",
      " 5   upvotes       7473 non-null   int64 \n",
      " 6   num_comments  7473 non-null   int64 \n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 467.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Filtering out the posts removed by redact.dev\n",
    "reddit_df = reddit_df[~reddit_df['body'].str.contains(re.escape(string_to_search))]\n",
    "\n",
    "# Checking updated dataframe\n",
    "reddit_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cleaning post content\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Removing hyperlinks and GIFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple posts contain links to other websites, including Giphy, a site to share GIF images. Running our Lemmatiser/WordVectoriser on such hyperlinks will not yield meaningful contributions to our model. We will thus remove these hyperlinks at this stage using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(338, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking number of rows with hyperlinks within 'body'\n",
    "test_link = reddit_df[reddit_df['body'].str.contains('http.*')]\n",
    "test_link.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking number of rows with giphy links within 'body'\n",
    "test_gif = reddit_df[reddit_df['body'].str.contains('![gif](giphy|', regex=False)]\n",
    "test_gif.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using regex to search the 'body' and remove hyperlinks from posts\n",
    "reddit_df['body'] = reddit_df['body'].str.replace('http[^ ]*', '', regex=True)\n",
    "\n",
    "# Check if any posts remain with hyperlinks\n",
    "test_link = reddit_df[reddit_df['body'].str.contains('http.*')]\n",
    "test_link.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aspire\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\strings\\object_array.py:172: FutureWarning: Possible nested set at position 4\n",
      "  pat = re.compile(pat, flags=flags)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using regex to search the 'body' and remove giphy links from posts\n",
    "reddit_df['body'] = reddit_df['body'].str.replace('[!][[]gif[]][^ ]*', '', regex=True)\n",
    "\n",
    "# Check if any posts remain with giphy links\n",
    "test_gif = reddit_df[reddit_df['body'].str.contains('![gif](giphy|', regex=False)]\n",
    "test_gif.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Converting Emojis to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scraping, emojis within posts are recorded as a coded alphanumerical string. We will use the demojize function from the emoji library to convert these strings to a textual format, which will give better context to the post when performing sentiment analysis and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subr-def_ai</th>\n",
       "      <th>is_op</th>\n",
       "      <th>author</th>\n",
       "      <th>post_id</th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1043ums</td>\n",
       "      <td>More ableism from the anti-AI crowd... Gross. 😟</td>\n",
       "      <td>193</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subr-def_ai  is_op     author  post_id  \\\n",
       "1116            1      1  [deleted]  1043ums   \n",
       "\n",
       "                                                 body  upvotes  num_comments  \n",
       "1116  More ableism from the anti-AI crowd... Gross. 😟      193             6  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing example of emoji within post\n",
    "reddit_df[reddit_df['post_id']=='1043ums']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the demojize function on 'body'\n",
    "reddit_df['body'] = reddit_df['body'].apply(demojize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subr-def_ai</th>\n",
       "      <th>is_op</th>\n",
       "      <th>author</th>\n",
       "      <th>post_id</th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1043ums</td>\n",
       "      <td>More ableism from the anti-AI crowd... Gross. :worried_face:</td>\n",
       "      <td>193</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subr-def_ai  is_op     author  post_id  \\\n",
       "1116            1      1  [deleted]  1043ums   \n",
       "\n",
       "                                                              body  upvotes  \\\n",
       "1116  More ableism from the anti-AI crowd... Gross. :worried_face:      193   \n",
       "\n",
       "      num_comments  \n",
       "1116             6  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing example of emoji after running demojize\n",
    "reddit_df[reddit_df['post_id']=='1043ums']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing empty posts after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the data cleaning steps above, certain rows have in the dataframe have become either empty, or filled with only newline characters. We will drop these rows in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace NaN values with an empty string for consistency\n",
    "reddit_df['body'] = reddit_df['body'].fillna('')\n",
    "\n",
    "# Check if any rows in 'body' consist of only '0', '', or only newline character\n",
    "rows_with_zeros_or_newline = reddit_df[(reddit_df['body'].isin(['0', '', '\\n', '\\n\\n', '\\n\\n\\n', '\\n\\n\\n\\n']))]\n",
    "\n",
    "# Count the rows that meet the condition\n",
    "rows_with_zeros_or_newline.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7443 entries, 0 to 7442\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   subr-def_ai   7443 non-null   int64 \n",
      " 1   is_op         7443 non-null   int64 \n",
      " 2   author        7443 non-null   object\n",
      " 3   post_id       7443 non-null   object\n",
      " 4   body          7443 non-null   object\n",
      " 5   upvotes       7443 non-null   int64 \n",
      " 6   num_comments  7443 non-null   int64 \n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 407.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Drop the rows with no actual text and reset the index\n",
    "reddit_df = reddit_df.drop(rows_with_zeros_or_newline.index).reset_index(drop=True)\n",
    "# Display the updated DataFrame info\n",
    "reddit_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature engineering\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will engineer the following features below to assist with EDA and modelling:\n",
    "\n",
    "\n",
    "|Feature|Type|Description|\n",
    "|---|---|---|\n",
    "|`post_length`|int|Character length of the post|\n",
    "|`post_word_count`|int|Number of words in the post|\n",
    "|`neg`|float|Measure of negativity of post*|\n",
    "|`neu`|float|Measure of neutrality of post*|\n",
    "|`pos`|float|Measure of positivity of post*|\n",
    "|`compound`|float|Compound score obtained from `neg`, `neu` and `pos`|\n",
    "|`subjectivity_score`|float|Measure of subjectivity of post|\n",
    "\n",
    "*These parameters will be generated using SentimentIntensityAnalyzer from the nltk.sentiment.vader library. Further documentation [here](https://www.nltk.org/howto/sentiment.html).\n",
    "\n",
    "**`subjectivity_score` is obtained using the TextBlob library. Further documentation [here](https://textblob.readthedocs.io/en/dev/quickstart.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Engineering `post_length` and `post_word_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering feature for the character length of each post\n",
    "reddit_df['post_length'] = reddit_df['body'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering feature for the number of words in each post\n",
    "reddit_df['post_word_count'] = reddit_df['body'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Engineering sentiment analysis features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subr-def_ai</th>\n",
       "      <th>is_op</th>\n",
       "      <th>author</th>\n",
       "      <th>post_id</th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_length</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>101n5dv</td>\n",
       "      <td>[TW: DEATH THREAT] And they say that \"AI bros\" are the ones harassing the artists?</td>\n",
       "      <td>498</td>\n",
       "      <td>9</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>{'neg': 0.385, 'neu': 0.615, 'pos': 0.0, 'compound': -0.8455}</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subr-def_ai  is_op     author  post_id  \\\n",
       "0            1      1  [deleted]  101n5dv   \n",
       "\n",
       "                                                                                 body  \\\n",
       "0  [TW: DEATH THREAT] And they say that \"AI bros\" are the ones harassing the artists?   \n",
       "\n",
       "   upvotes  num_comments  post_length  post_word_count  \\\n",
       "0      498             9           82               15   \n",
       "\n",
       "                                                sentiment_scores    neg  \\\n",
       "0  {'neg': 0.385, 'neu': 0.615, 'pos': 0.0, 'compound': -0.8455}  0.385   \n",
       "\n",
       "     neu  pos  compound  \n",
       "0  0.615  0.0   -0.8455  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate Sentiment Intensity Analyzer\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply sentiment analysis to the 'body' column\n",
    "reddit_df['sentiment_scores'] = reddit_df['body'].apply(lambda x: sent.polarity_scores(x))\n",
    "\n",
    "# Expand the sentiment scores into separate columns\n",
    "sentiment_df = reddit_df['sentiment_scores'].apply(pd.Series)\n",
    "\n",
    "# Concatenate the sentiment scores DataFrame with the original DataFrame\n",
    "reddit_df = pd.concat([reddit_df, sentiment_df], axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "reddit_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get subjectivity score using TextBlob\n",
    "def get_subjectivity_score(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subr-def_ai</th>\n",
       "      <th>is_op</th>\n",
       "      <th>author</th>\n",
       "      <th>post_id</th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_length</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>subjectivity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>101n5dv</td>\n",
       "      <td>[TW: DEATH THREAT] And they say that \"AI bros\" are the ones harassing the artists?</td>\n",
       "      <td>498</td>\n",
       "      <td>9</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8455</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subr-def_ai  is_op     author  post_id  \\\n",
       "0            1      1  [deleted]  101n5dv   \n",
       "\n",
       "                                                                                 body  \\\n",
       "0  [TW: DEATH THREAT] And they say that \"AI bros\" are the ones harassing the artists?   \n",
       "\n",
       "   upvotes  num_comments  post_length  post_word_count    neg    neu  pos  \\\n",
       "0      498             9           82               15  0.385  0.615  0.0   \n",
       "\n",
       "   compound  subjectivity_score  \n",
       "0   -0.8455                 0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply subjectivity analysis to the 'body' column\n",
    "reddit_df['subjectivity_score'] = reddit_df['body'].apply(get_subjectivity_score)\n",
    "\n",
    "# Drop unnecessary 'sentiment_scores' column after splitting\n",
    "reddit_df.drop(columns=['sentiment_scores'], inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "reddit_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        subreddit       neg       neu       pos  compound  subjectivity_score\n",
      "0      ArtistHate  0.092619  0.775681  0.130045  0.114222            0.467722\n",
      "1  DefendingAIArt  0.099033  0.775771  0.124971  0.060208            0.439919\n"
     ]
    }
   ],
   "source": [
    "# Group by 'subr-def_ai' and calculate average scores\n",
    "average_scores = reddit_df.groupby('subr-def_ai').agg({\n",
    "    'neg': 'mean',\n",
    "    'neu': 'mean',\n",
    "    'pos': 'mean',\n",
    "    'compound': 'mean',\n",
    "    'subjectivity_score': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Add labels for better readability\n",
    "average_scores['subreddit'] = average_scores['subr-def_ai'].map({0: 'ArtistHate', 1: 'DefendingAIArt'})\n",
    "\n",
    "# Reorder columns for better display\n",
    "average_scores = average_scores[['subreddit', 'neg', 'neu', 'pos', 'compound', 'subjectivity_score']]\n",
    "\n",
    "# Display the average scores\n",
    "print(average_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Class Imbalance Analysis\n",
    "___\n",
    "\n",
    "- For our project to accurately predict the subreddit to which a particular text belongs, we have noted a noteworthy class imbalance, especially within posts containing fewer than 50 words in the *r/DefendingAIArt* subreddit. Recognizing the importance of obtaining meaningful insights from our data, we have conducted a thorough analysis and decided to address this imbalance.\n",
    "\n",
    "- In the context of our broader goal, where *r/DefendingAIArt* is associated with pro AI art sentiments and *r/ArtistHate* with anti AI art sentiments, we understand the significance of each comment contributing substantively to our model's training and evaluation. Comments with minimal word counts are likely to provide limited information and context, potentially affecting the accuracy of our predictions.\n",
    "\n",
    "- To enhance the quality of our predictive model, we have opted to selectively remove rows from the *r/DefendingAIArt* subreddit, focusing on those with the lowest word count. This strategic approach ensures that our model is trained on more informative and context-rich comments, fostering a better understanding of the sentiments expressed within each subreddit.\n",
    "\n",
    "- By achieving a balanced representation between *r/DefendingAIArt* and *r/ArtistHate*, our predictive model aims to accurately discern the subtle nuances between pro and anti AI sentiments, ultimately improving its performance in distinguishing between the two subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Balance (Normalized):\n",
      "subr-def_ai\n",
      "DefendingAIArt    0.594921\n",
      "ArtistHate        0.405079\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class Balance (Actual Counts):\n",
      "subr-def_ai\n",
      "DefendingAIArt    4428\n",
      "ArtistHate        3015\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate class balance\n",
    "class_balance = reddit_df['subr-def_ai'].value_counts(normalize=True)\n",
    "class_counts = reddit_df['subr-def_ai'].value_counts()\n",
    "\n",
    "# Define class labels\n",
    "class_labels = {1: 'DefendingAIArt', 0: 'ArtistHate'}\n",
    "\n",
    "# Rename the indices using the class labels\n",
    "class_balance.index = class_balance.index.map(class_labels)\n",
    "class_counts.index = class_counts.index.map(class_labels)\n",
    "\n",
    "# Print out the class balance with labels\n",
    "print(\"Class Balance (Normalized):\")\n",
    "print(class_balance)\n",
    "\n",
    "# Print out the actual value counts with labels\n",
    "print(\"\\nClass Balance (Actual Counts):\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Class Balance (Actual Counts):\n",
      "subr-def_ai\n",
      "1    3015\n",
      "0    3015\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Desired class balance\n",
    "desired_balance = 3015\n",
    "\n",
    "# Sort the rows for 'DefendingAIArt' based on 'status_word_count'\n",
    "defending_ai_sorted = reddit_df[reddit_df['subr-def_ai'] == 1].sort_values(by='post_word_count')\n",
    "\n",
    "# Calculate the number of rows to drop\n",
    "rows_to_drop = len(defending_ai_sorted) - desired_balance\n",
    "\n",
    "# Drop the excess rows\n",
    "reddit_df.drop(index=defending_ai_sorted.head(rows_to_drop).index, inplace=True)\n",
    "\n",
    "# Verify the new class balance\n",
    "new_class_balance = reddit_df['subr-def_ai'].value_counts()\n",
    "print(\"New Class Balance (Actual Counts):\")\n",
    "print(new_class_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6030, 14)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting to csv format for EDA and modelling step\n",
    "reddit_df.to_csv('../data/reddit_df_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(DEPRECATED)** Sentiment Analysis using sst2\n",
    "\n",
    "**The code in this section been deprecated in favour of using SentimentIntensityAnalyzer(), which does not have a character limit and has a faster run time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before developing the prediction model, we will engineer features that capture sentiment, using sentiment analysis on each post.\n",
    "\n",
    "For the purpose of this project, we will not be creating a unique sentiment analysis model. Instead, we will use the default sentiment analysis model provided by Huggingface, [sst2](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
    "\n",
    "Unfortunately, sst2 does not work on character strings longer than 1300 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_df[reddit_df['post_length']>1300].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posts identified above exceed this character limit. We will filter out posts from our dataframe where post length is greater than 1300 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out posts from the dataframe where 'post_length' > 1300\n",
    "\n",
    "# reddit_df = reddit_df.loc[reddit_df['post_length']<1300, :].reset_index(drop=True)\n",
    "# reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise sst2 sentiment analysis model\n",
    "\n",
    "# sent_an = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# body_list = [x for x in reddit_df['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent = []\n",
    "# sent_score = []\n",
    "# for body in body_list:\n",
    "#     result = sent_an(body)\n",
    "#     sent.append(result[0]['label'])\n",
    "#     sent_score.append(result[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_df['sent'] = pd.DataFrame(sent, columns=['sent'])\n",
    "# reddit_df['sent_score'] = pd.DataFrame(sent_score, columns=['sent_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_df.to_csv('data/reddit_engdf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_df.groupby(['subr-def_ai']).mean(['sent_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
